{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ZO-optimization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import logisticReg\n",
    "from simus import run_log\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import load_breast_cancer,load_diabetes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool function to simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without intercept\n",
    "def simu_block(seed,n_samples,n_features,puiss,block_size,noise):\n",
    "    np.random.seed(seed)\n",
    "    X = np.zeros((n_samples,n_features))\n",
    "    for j in range(n_features//block_size):\n",
    "        X_j = np.random.normal(scale=(j+1)**(-puiss),size=(n_samples,block_size))\n",
    "        X[:,j*block_size:(j+1)*block_size] = X_j\n",
    "    # shuffle columns of X\n",
    "    indices = np.arange(n_features)\n",
    "    np.random.shuffle(indices)\n",
    "    X[:,:] = X[:, indices]\n",
    "    ground_truth = np.random.uniform(low=0,high=1,size=n_features)\n",
    "    y = np.ones(n_samples)\n",
    "    h = 1/(1+np.exp(-X@ground_truth))\n",
    "    if noise > 0.0:\n",
    "        h += np.random.normal(scale=noise, size=y.shape)\n",
    "    y[h<=0.5]=-1\n",
    "    #indices = np.arange(n_features)\n",
    "    #np.random.shuffle(indices)\n",
    "    #X[:,:] = X[:, indices]\n",
    "    # Add noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-64.0\n"
     ]
    }
   ],
   "source": [
    "puiss=5\n",
    "seed=0\n",
    "noise=0.01\n",
    "block_size=5\n",
    "N = int(200)  # number of passes over coordinates          \n",
    "a = 10           # numerator of learning rate\n",
    "t0 = 5\n",
    "alpha_power = 1 # power in the learning rate\n",
    "gamma = 1       # numerator in gradient factor smoothing\n",
    "mu_power = 1    # power in the gradient factor smoothing\n",
    "verbose = False # to display information\n",
    "N_exp = 20     # number of experiments\n",
    "fixed=False\n",
    "eta = 0.5\n",
    "#fixed = True\n",
    "batch_size = 1\n",
    "# Parameters\n",
    "for n_samples in [1000,2000,5000]:\n",
    "    for n_features in [20,50,100,200]:\n",
    "        T = int(np.sqrt(n_features)) # size of exploration\n",
    "        X,y=simu_block(seed=seed,n_samples=n_samples,n_features=n_features,\n",
    "               puiss=puiss,block_size=block_size,noise=noise)\n",
    "        Î» = 1/X.shape[0]\n",
    "        c = 1/(X.shape[0]*Î»)\n",
    "        log_sk = LogisticRegression(C=c,fit_intercept=False,tol=1e-6)\n",
    "        # fit sklearn model\n",
    "        log_sk.fit(X=X,y=y)\n",
    "        coeff = log_sk.coef_[0]\n",
    "        data_term  = np.log(1+np.exp(np.multiply(-y,X@coeff))).mean()\n",
    "        reg_term = (ðœ†/2)*sum(coeff**2)\n",
    "        loss_opt = data_term + reg_term\n",
    "        _,_,loss_full = run_log(X=X,y=y,ðœ†=ðœ†,fit_intercept=False,batch_size=batch_size,method='full',N_exp=N_exp,N=N,\n",
    "                   T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                   fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "        _,_,loss_uni = run_log(X=X,y=y,ðœ†=ðœ†,batch_size=batch_size,fit_intercept=False,\n",
    "                               method='uni',N_exp=N_exp,N=N,\n",
    "                           T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                        fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "        _,_,loss_avg = run_log(X=X,y=y,ðœ†=ðœ†,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='avg',\n",
    "                       verbose=False)\n",
    "        _,_,loss_mus_abs = run_log(X=X,y=y,ðœ†=ðœ†,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='abs',\n",
    "                       verbose=False)\n",
    "        _,_,loss_mus_sqr = run_log(X=X,y=y,ðœ†=ðœ†,fit_intercept=False,batch_size=batch_size,\n",
    "                               method='mus',N_exp=N_exp,N=N,\n",
    "                               T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                               fixed=fixed,eta=eta,importance=False,gains='square',\n",
    "                               verbose=False)\n",
    "        _,_,loss_nes = run_log(X=X,y=y,ðœ†=ðœ†,fit_intercept=False,batch_size=batch_size,\n",
    "                               method='nes',N_exp=N_exp,N=N,\n",
    "                               T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                               fixed=fixed,eta=eta,importance=False,gains='square',\n",
    "                               verbose=False)\n",
    "        np.save('results_log/loss_ful_log_n{}_p{}.npy'.format(n_samples,n_features),loss_full-loss_opt)\n",
    "        np.save('results_log/loss_uni_log_n{}_p{}.npy'.format(n_samples,n_features),loss_uni-loss_opt)\n",
    "        np.save('results_log/loss_nes_log_n{}_p{}.npy'.format(n_samples,n_features),loss_nes-loss_opt)\n",
    "        np.save('results_log/loss_avg_log_n{}_p{}.npy'.format(n_samples,n_features),loss_avg-loss_opt)\n",
    "        np.save('results_log/loss_sqr_log_n{}_p{}.npy'.format(n_samples,n_features),loss_sqr-loss_opt)\n",
    "        np.save('results_log/loss_abs_log_n{}_p{}.npy'.format(n_samples,n_features),loss_abs-loss_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
