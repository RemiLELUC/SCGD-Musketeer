{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> ZO-Logistic Regression\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from models import logisticReg\n",
    "from simus import run_log\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without intercept\n",
    "def simu_block(seed,n_samples,n_features,puiss,block_size,noise):\n",
    "    np.random.seed(seed)\n",
    "    X = np.zeros((n_samples,n_features))\n",
    "    for j in range(n_features//block_size):\n",
    "        X_j = np.random.normal(scale=(j+1)**(-puiss),size=(n_samples,block_size))\n",
    "        X[:,j*block_size:(j+1)*block_size] = X_j\n",
    "    # shuffle columns of X\n",
    "    indices = np.arange(n_features)\n",
    "    np.random.shuffle(indices)\n",
    "    X[:,:] = X[:, indices]\n",
    "    ground_truth = np.random.uniform(low=0,high=1,size=n_features)\n",
    "    y = np.ones(n_samples)\n",
    "    h = 1/(1+np.exp(-X@ground_truth))\n",
    "    if noise > 0.0:\n",
    "        h += np.random.normal(scale=noise, size=y.shape)\n",
    "    y[h<=0.5]=-1\n",
    "    #indices = np.arange(n_features)\n",
    "    #np.random.shuffle(indices)\n",
    "    #X[:,:] = X[:, indices]\n",
    "    # Add noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puiss=5\n",
    "# Parameters\n",
    "n_samples = 10000   # number of samples\n",
    "n_features = 250    # dimension of the problem\n",
    " = 1/(n_samples)#regularization parameter\n",
    "# Simulate data for regression\n",
    "seed=0\n",
    "noise=0.01\n",
    "block_size=5\n",
    "X,y=simu_block(seed=seed,n_samples=n_samples,n_features=n_features,\n",
    "               puiss=puiss,block_size=block_size,noise=noise)\n",
    "print(y.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute true solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "位 = 1/X.shape[0]\n",
    "c = 1/(X.shape[0]*位)\n",
    "log_sk = LogisticRegression(C=c,fit_intercept=False,tol=1e-6)\n",
    "# fit sklearn model\n",
    "log_sk.fit(X=X,y=y)\n",
    "coeff = log_sk.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_term  = np.log(1+np.exp(np.multiply(-y,X@coeff))).mean()\n",
    "reg_term = (/2)*sum(coeff**2)\n",
    "print('data_term:',data_term)\n",
    "print('reg_term :',reg_term)\n",
    "# Optimal loss\n",
    "log = logisticReg(X=X,y=y,位=位,fit_intercept=False)\n",
    "loss_opt = log.loss(batch=np.arange(X.shape[0]),w=coeff)\n",
    "print('loss_opt :',loss_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples,n_features = X.shape\n",
    "N = int(200)  # number of passes over coordinates          \n",
    "a = 10           # numerator of learning rate\n",
    "t0 = 5\n",
    "alpha_power = 1 # power in the learning rate\n",
    "gamma = 1       # numerator in gradient factor smoothing\n",
    "mu_power = 1    # power in the gradient factor smoothing\n",
    "verbose = False # to display information\n",
    "N_exp = 20     # number of experiments\n",
    "fixed=False\n",
    "eta = 0.5\n",
    "#fixed = True\n",
    "batch_size = 1\n",
    "\n",
    "print('=  ',)\n",
    "print('eta=',eta)\n",
    "T = int(np.sqrt(n_features)) # size of exploration\n",
    "#T = n_features\n",
    "print('T=  ',T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run different ZO methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full gradient estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,loss_full = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,method='full',N_exp=N_exp,N=N,\n",
    "                   T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                   fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "l_ful = np.mean(loss_full,axis=0)\n",
    "std_ful = np.std(loss_full,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform coordinate sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,loss_uni = run_log(X=X,y=y,=,batch_size=batch_size,fit_intercept=False,\n",
    "                               method='uni',N_exp=N_exp,N=N,\n",
    "                           T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                        fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "l_uni = np.mean(loss_uni,axis=0)\n",
    "std_uni = np.std(loss_uni,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musketeer biased (with different gains: Average, Absolute Value, Square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,loss_mus_avg = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='avg',\n",
    "                       verbose=False)\n",
    "l_avg = np.mean(loss_mus_avg,axis=0)\n",
    "std_avg = np.std(loss_mus_avg,axis=0)\n",
    "\n",
    "\n",
    "_,_,loss_mus_abs = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='abs',\n",
    "                       verbose=False)\n",
    "l_abs = np.mean(loss_mus_abs,axis=0)\n",
    "std_abs = np.std(loss_mus_abs,axis=0)\n",
    "\n",
    "_,_,loss_mus_sqr = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='square',\n",
    "                       verbose=False)\n",
    "l_sqr = np.mean(loss_mus_sqr,axis=0)\n",
    "std_sqr = np.std(loss_mus_sqr,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian smoothing estimate (Nesterov-Spokoiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,loss_nes = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='nes',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=False,gains='square',\n",
    "                       verbose=False)\n",
    "l_nes = np.mean(loss_nes,axis=0)\n",
    "std_nes = np.std(loss_nes,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('loss_ful_log_zo_puiss_{}.npy'.format(puiss),loss_full)\n",
    "#np.save('loss_uni_log_zo_puiss_{}.npy'.format(puiss),loss_uni)\n",
    "#np.save('loss_avg_log_zo_puiss_{}.npy'.format(puiss),loss_mus_avg)\n",
    "#np.save('loss_abs_log_zo_puiss_{}.npy'.format(puiss),loss_mus_abs)\n",
    "#np.save('loss_sqr_log_zo_puiss_{}.npy'.format(puiss),loss_mus_sqr)\n",
    "#np.save('loss_nes_log_zo_puiss_{}.npy'.format(puiss),loss_nes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiments in different settings (Appendix E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puiss=5\n",
    "seed=0\n",
    "noise=0.01\n",
    "block_size=5\n",
    "N = int(200)  # number of passes over coordinates          \n",
    "a = 10           # numerator of learning rate\n",
    "t0 = 5\n",
    "alpha_power = 1 # power in the learning rate\n",
    "gamma = 1       # numerator in gradient factor smoothing\n",
    "mu_power = 1    # power in the gradient factor smoothing\n",
    "verbose = False # to display information\n",
    "N_exp = 20     # number of experiments\n",
    "fixed=False\n",
    "eta = 0.5\n",
    "#fixed = True\n",
    "batch_size = 1\n",
    "# Parameters\n",
    "for n_samples in [1000,2000,5000]:\n",
    "    for n_features in [20,50,100,200]:\n",
    "        print('n=',n_samples)\n",
    "        print('p=',n_features)\n",
    "        T = int(np.sqrt(n_features)) # size of exploration\n",
    "        # Generate data for classification\n",
    "        X,y=simu_block(seed=seed,n_samples=n_samples,n_features=n_features,\n",
    "               puiss=puiss,block_size=block_size,noise=noise)\n",
    "        位 = 1/X.shape[0]\n",
    "        c = 1/(X.shape[0]*位)\n",
    "        log_sk = LogisticRegression(C=c,fit_intercept=False,tol=1e-6)\n",
    "        # fit sklearn model\n",
    "        log_sk.fit(X=X,y=y)\n",
    "        coeff = log_sk.coef_[0]\n",
    "        data_term  = np.log(1+np.exp(np.multiply(-y,X@coeff))).mean()\n",
    "        reg_term = (/2)*sum(coeff**2)\n",
    "        loss_opt = data_term + reg_term\n",
    "        # Run different ZO methods for Logistic regression\n",
    "        _,_,loss_full = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,method='full',N_exp=N_exp,N=N,\n",
    "                   T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                   fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "        _,_,loss_uni = run_log(X=X,y=y,=,batch_size=batch_size,fit_intercept=False,\n",
    "                               method='uni',N_exp=N_exp,N=N,\n",
    "                           T=None,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                        fixed=None,eta=None,importance=None,gains=None,verbose=False)\n",
    "        _,_,loss_avg = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=True,gains='avg',\n",
    "                       verbose=False)\n",
    "        _,_,loss_abs = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                       method='mus',N_exp=N_exp,N=N,\n",
    "                       T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                       fixed=fixed,eta=eta,importance=True,gains='abs',\n",
    "                       verbose=False)\n",
    "        _,_,loss_sqr = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                               method='mus',N_exp=N_exp,N=N,\n",
    "                               T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                               fixed=fixed,eta=eta,importance=True,gains='square',\n",
    "                               verbose=False)\n",
    "        _,_,loss_nes = run_log(X=X,y=y,=,fit_intercept=False,batch_size=batch_size,\n",
    "                               method='nes',N_exp=N_exp,N=N,\n",
    "                               T=T,gamma=gamma,mu_power=mu_power,a=a,t0=t0,alpha_power=alpha_power,\n",
    "                               fixed=fixed,eta=eta,importance=False,gains='square',\n",
    "                               verbose=False)\n",
    "        #np.save('loss_ful_log_n{}_p{}.npy'.format(n_samples,n_features),loss_full-loss_opt)\n",
    "        #np.save('loss_uni_log_n{}_p{}.npy'.format(n_samples,n_features),loss_uni-loss_opt)\n",
    "        #np.save('loss_nes_log_n{}_p{}.npy'.format(n_samples,n_features),loss_nes-loss_opt)\n",
    "        #np.save('loss_avg_is_log_n{}_p{}.npy'.format(n_samples,n_features),loss_avg-loss_opt)\n",
    "        #np.save('loss_sqr_is_log_n{}_p{}.npy'.format(n_samples,n_features),loss_sqr-loss_opt)\n",
    "        #np.save('loss_abs_is_log_n{}_p{}.npy'.format(n_samples,n_features),loss_abs-loss_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
